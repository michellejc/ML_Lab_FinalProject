{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mental Health and the Tech Industry\n",
    "----------\n",
    "#### Research Question\n",
    "Can we predicting if an individual has sought treatment for a mental health condition based on a variety of mental health realted survey responses\n",
    "\n",
    "Data from Kaggle: https://www.kaggle.com/osmi/mental-health-in-tech-survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports \n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from   sklearn.compose         import *\n",
    "from   sklearn.experimental    import enable_iterative_imputer\n",
    "from   sklearn.impute          import *\n",
    "from   sklearn.compose         import TransformedTargetRegressor \n",
    "from   sklearn.pipeline        import Pipeline\n",
    "from   sklearn.preprocessing   import *\n",
    "from   sklearn.preprocessing   import LabelEncoder\n",
    "from   sklearn.preprocessing   import StandardScaler\n",
    "from   sklearn.base            import BaseEstimator\n",
    "from   sklearn.ensemble        import RandomForestClassifier\n",
    "from   sklearn.linear_model    import LogisticRegression \n",
    "from   sklearn.linear_model    import RidgeClassifier\n",
    "from   sklearn.model_selection import RandomizedSearchCV\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.neighbors       import KNeighborsClassifier\n",
    "from   sklearn.svm             import SVC\n",
    "from   sklearn.model_selection import cross_validate\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.metrics         import f1_score\n",
    "from   sklearn.metrics         import roc_auc_score\n",
    "from   sklearn.metrics         import roc_curve\n",
    "from   sklearn.inspection      import permutation_importance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'https://raw.githubusercontent.com/michellejc/ML_Lab_FinalProject/main/survey.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squestering Test Set\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['treatment']\n",
    "X = df.drop(columns=['treatment'])\n",
    "\n",
    "X_train_main, X_test, y_train_main, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Target Variable \n",
    "----\n",
    "Transforming the target variable y (has or has not sought treatment) from \"yes\" or \"no\" to 0 or 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(y_train_main)\n",
    "\n",
    "y_train_main = le.transform(y_train_main)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Preprocessing Pipeline \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical pipeline\n",
    "bin_column = ['Age']\n",
    "categorical_columns = ['Gender', 'Country', 'state', 'self_employed',\n",
    "       'family_history', 'work_interfere', 'remote_work',\n",
    "       'tech_company', 'benefits', 'care_options', 'wellness_program',\n",
    "       'seek_help', 'anonymity', 'leave', 'mental_health_consequence',\n",
    "       'phys_health_consequence', 'coworkers', 'supervisor',\n",
    "       'mental_health_interview', 'phys_health_interview',\n",
    "       'mental_vs_physical']\n",
    "\n",
    "# binning age \n",
    "bin_pipe = Pipeline([('imputer', SimpleImputer(strategy='median')),\n",
    "                     ('bin', KBinsDiscretizer(n_bins=7, encode='onehot'))])\n",
    "\n",
    "# categorical pipeline \n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                     ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# combining preprocessing steps into one \n",
    "preprocessing = ColumnTransformer([('categorical', cat_pipe, categorical_columns),\n",
    "                                   ('bin', bin_pipe, bin_column)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics \n",
    "----\n",
    "I selected the F1 measure as my first evaluation metric. I choose the F1 score because it balances precision and recall for the binary classification, which I feel is appropriate because I do not wish to prioritize precision or recall over the other. Furthermore it's values are always between 0 and 1 with values closer to 1 being better, which I feel is a simple and easy to interpret measure of this model. \n",
    "\n",
    "The second metric I track is the ROC score, or area under the Reciever Operating Characteristic curve. I chose this metric because it also captures a balance between precision and recall - prioritizing both a high true positive rate and a low false positive rate.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized CV Search to select candidate models\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper class\n",
    "class DummyEstimator(BaseEstimator):\n",
    "    \"Pass through class, methods are present but do nothing.\"\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier - f1: 0.82, roc: 0.81\n",
      "LogisticRegression - f1: 0.82, roc: 0.81\n",
      "RidgeClassifier   - f1: 0.83, roc: 0.81\n",
      "KNeighborsClassifier - f1: 0.73, roc: 0.74\n",
      "SVC               - f1: 0.83, roc: 0.81\n"
     ]
    }
   ],
   "source": [
    "algorithms = [RandomForestClassifier(),\n",
    "             LogisticRegression(),\n",
    "             RidgeClassifier(),\n",
    "             KNeighborsClassifier(),\n",
    "             SVC()]\n",
    "\n",
    "for algo in algorithms: \n",
    "    pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                     ('classifier',  algo)])\n",
    "    f1_list = []\n",
    "    roc_list = []\n",
    "    \n",
    "    for x in range(20):\n",
    "        X_train, X_validate, y_train, y_validate = train_test_split(X_train_main, y_train_main)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_validate)\n",
    "        f1 = f1_score(y_validate, y_pred)\n",
    "        f1_list.append(f1)\n",
    "        \n",
    "        roc = roc_auc_score(y_validate, y_pred)\n",
    "        roc_list.append(roc)\n",
    "    \n",
    "    average_f1 = sum(f1_list)/len(f1_list)\n",
    "    average_roc = sum(roc_list)/len(roc_list)\n",
    "        \n",
    "    print(f\"{algo.__class__.__name__:<17} - f1: {average_f1:,.2f}, roc: {average_roc:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate 1: SVC\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'classifier__C': [1.0,2,3,4,5],\n",
    "                     'classifier__break_ties': [False, True],\n",
    "                     'classifier__cache_size': [200],\n",
    "                     'classifier__class_weight': [None],\n",
    "                     'classifier__coef0': [0.0],\n",
    "                     'classifier__decision_function_shape': ['ovr'],\n",
    "                     'classifier__degree': [3,4,5,6,7],\n",
    "                     'classifier__kernel': ['rbf','linear', 'poly', 'sigmoid', 'precomputed'],\n",
    "                     'classifier__probability': [False, True],\n",
    "                     'classifier__shrinking': [True, False],\n",
    "                     'classifier__tol': [0.001, 0.01, 0.002, 0.01],\n",
    "                     'classifier__verbose': [False]}\n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  SVC())])\n",
    "\n",
    "\n",
    "clf_rand_cv = RandomizedSearchCV(estimator=pipe, \n",
    "                              param_distributions=hyperparameters, \n",
    "                              n_iter=25,\n",
    "                              cv=5, \n",
    "                              n_jobs=-1,\n",
    "                              verbose=False)\n",
    "\n",
    "best_model_svc = clf_rand_cv.fit(X_train_main, y_train_main) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 scores 0.84 with a standard deviation of 0.02\n",
      "ROC scores 0.87 with a standard deviation of 0.03\n"
     ]
    }
   ],
   "source": [
    "scoring = ['f1', 'roc_auc']\n",
    "scores = cross_validate(best_model_svc, X_train_main, y_train_main, cv=5, scoring=scoring)\n",
    "\n",
    "f1_scores = scores['test_f1']\n",
    "roc_auc_scores = scores['test_roc_auc']\n",
    "print(\"f1 scores %0.2f with a standard deviation of %0.2f\" % (f1_scores.mean(), f1_scores.std()))\n",
    "print(\"ROC scores %0.2f with a standard deviation of %0.2f\" % (roc_auc_scores.mean(), roc_auc_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate 2: RidgeClassifier\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'classifier__alpha': [1.0, 2,3,4,5],\n",
    "                    'classifier__class_weight': [None, 'balanced'],\n",
    "                    'classifier__copy_X': [True, False],\n",
    "                    'classifier__normalize': [False, True],\n",
    "                    'classifier__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  RidgeClassifier())])\n",
    "\n",
    "\n",
    "clf_rand_cv = RandomizedSearchCV(estimator=pipe, \n",
    "                              param_distributions=hyperparameters, \n",
    "                              n_iter=25,\n",
    "                              cv=5, \n",
    "                              n_jobs=-1,\n",
    "                              verbose=False)\n",
    "\n",
    "best_model_ridge = clf_rand_cv.fit(X_train_main, y_train_main) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 scores 0.82 with a standard deviation of 0.03\n",
      "ROC scores 0.88 with a standard deviation of 0.02\n"
     ]
    }
   ],
   "source": [
    "scoring = ['f1', 'roc_auc']\n",
    "scores = cross_validate(best_model_ridge, X_train_main, y_train_main, cv=5, scoring=scoring)\n",
    "\n",
    "f1_scores = scores['test_f1']\n",
    "roc_auc_scores = scores['test_roc_auc']\n",
    "print(\"f1 scores %0.2f with a standard deviation of %0.2f\" % (f1_scores.mean(), f1_scores.std()))\n",
    "print(\"ROC scores %0.2f with a standard deviation of %0.2f\" % (roc_auc_scores.mean(), roc_auc_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate 3: Random Forest Classifier\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'classifier__bootstrap': [True, False],\n",
    "     'classifier__ccp_alpha': [1.0, 2.0, 0.0],\n",
    "     'classifier__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "     'classifier__criterion': ['gini', 'entropy'],\n",
    "     'classifier__max_depth': [None,2,3,4,5],\n",
    "     'classifier__max_features': ['auto'],\n",
    "     'classifier__max_leaf_nodes':[None, 2,3,4,5],\n",
    "     'classifier__max_samples': [None,2,3,4,5],\n",
    "     'classifier__min_impurity_decrease': [0.0],\n",
    "     'classifier__min_impurity_split': [None],\n",
    "     'classifier__min_samples_leaf': [1,2,3,4,5],\n",
    "     'classifier__min_samples_split': [2,3,4,5,6],\n",
    "     'classifier__min_weight_fraction_leaf': [0.0],\n",
    "     'classifier__n_estimators': [10, 50, 100, 110, 120, 200],\n",
    "     'classifier__oob_score': [False, True],\n",
    "     'classifier__warm_start': [False, True]}\n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  RandomForestClassifier())])\n",
    "\n",
    "\n",
    "clf_rand_cv = RandomizedSearchCV(estimator=pipe, \n",
    "                              param_distributions=hyperparameters, \n",
    "                              n_iter=25,\n",
    "                              cv=5, \n",
    "                              n_jobs=-1,\n",
    "                              verbose=False)\n",
    "\n",
    "best_model_rf = clf_rand_cv.fit(X_train_main, y_train_main) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['f1', 'roc_auc']\n",
    "scores = cross_validate(best_model_rf, X_train_main, y_train_main, cv=5, scoring=scoring)\n",
    "\n",
    "f1_scores = scores['test_f1']\n",
    "roc_auc_scores = scores['test_roc_auc']\n",
    "print(\"f1 scores %0.2f with a standard deviation of %0.2f\" % (f1_scores.mean(), f1_scores.std()))\n",
    "print(\"ROC scores %0.2f with a standard deviation of %0.2f\" % (roc_auc_scores.mean(), roc_auc_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Model: Ridge Classification \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = best_model_ridge\n",
    "y_pred = final_model.predict(X_test)\n",
    "ROC_final_score = roc_auc_score(y_test, y_pred)\n",
    "F1_final_score = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Final f1 score: {F1_final_score} \\nFinal ROC score {ROC_final_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the permutation feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = permutation_importance(final_model, \n",
    "                           X_test, y_test,  \n",
    "                           n_repeats=10)\n",
    "\n",
    "importances = r.importances_mean\n",
    "features = X.columns\n",
    "\n",
    "feat_importances = []\n",
    "for feature in zip(features, importances):\n",
    "    feat_importances.append(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selecting the top ten important features in this model to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_df = pd.DataFrame(feat_importances)\n",
    "importances_df.columns = ['feature', 'importance']\n",
    "importances_df = importances_df.sort_values(by=['importance'], ascending=False)\n",
    "importances_df= importances_df.drop([3,7,21,25,20,24, 16,4,5,18,15,14,23,22,12,11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(45,15)) \n",
    "\n",
    "plt.bar(importances_df['feature'], importances_df['importance'], color='#ffd488')\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.set_title(\"Top 10 Features by Permutation Importance\", fontsize=50)\n",
    "ax.set_xticklabels(importances_df['feature'], fontsize = '22', color = '#525252', fontweight='bold')\n",
    "ax.set_yticklabels(importances_df['importance'], fontsize = '22', color = '#525252', fontweight = 'bold')\n",
    "\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
